# 迁移学习

## fine-tuning

核心思想：

深度学习可以分成两个阶段：特征提取和分类

如果在一个大的复杂的数据集上进行训练，那么模型可以学习到一些通用的特征，这些特征可以用于其他任务。

那么我们就可以把在大的数据集上训练的模型，在小的数据集上进行微调，从而得到一个更好的模型。

### 数据集选择

源数据集和目标数据集的分布要尽可能的相似，否则微调的效果会不好。

> 现在公司都把在大的数据集上预训练的模型当作自己的财产，不会轻易的分享出来。

- 预训练模型的质量非常重要；微调通常速度更快、精度更高
- 源数据集远复杂于目标数据，通常微调效果更好

### 全量微调

- 使用更强的正则化
  - 使用更小的学习率
  - 使用更少的数据迭代

### 部分微调

- 底部层的特征更加通用，高层的特征与数据集与任务更相关，所以可以固定底部一些层的参数，不参与更新。

## Adapter

Adapt Tuning。即在模型中添加 Adapter 层，在微调时冻结原参数，仅更新 Adapter 层。

具体而言，其在预训练模型每层中插入用于下游任务的参数，即 Adapter 模块，在微调时冻结模型主体，仅训练特定于任务的参数
